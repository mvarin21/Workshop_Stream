{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This is only applicable to Rhino. The principles of parallel computing are general and can be used to accelerate code on any system, but the implementations here are designed to interact with the scheduler used on the Computational Memory Lab cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parallel Computing with Dask\n",
    "\n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/styles/with_sidebar_1_up/public/nodesNetwork.gif?itok=TBqDQmx0\">\n",
    "\n",
    "A node is a like a computer within a much bigger computer!\n",
    "\n",
    "**CMLDask wrapper**:\n",
    "* See https://github.com/pennmem/cmldask for wrapper package created for convenience using Rhino + typical parallel setup for our analyses\n",
    "* Check out [Dask](https://docs.dask.org/en/stable/) and [Dask distributed](https://distributed.dask.org/en/stable/) for more documentation about the underlying implementation. In particular, we're using the [Futures API](https://docs.dask.org/en/stable/futures.html?highlight=futures) for managing parallel tasks and [dask-jobqueue](http://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SGECluster.html#dask_jobqueue.SGECluster) to connect with the Sun Grid Engine (SGE) scheduler on rhino. \n",
    "* Note that this package is intended for convenience and will not accomodate very specific computational needs that demand complex parallel architectures. You will need to use dask directly and create your own `Client()` instances that are more closely tailored to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmldask.CMLDask as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for ryan.colyer is 51391\n",
      "{'dashboard_address': ':51391'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN ryan.colyer@rhino2.psych.upenn.edu -L 8000:192.168.86.143:51391` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to localhost:8000 in your browser\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cmldask\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "\n",
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "logdir = os.path.join(os.path.abspath(os.curdir), 'dask_appendix_logs')\n",
    "first_run = False\n",
    "if not os.path.exists(logdir):\n",
    "  os.mkdir(logdir)\n",
    "  first_run = True\n",
    "client = da.new_dask_client_slurm(\"test_dask\", \"1GB\", log_directory=logdir, max_n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home1/ryan.colyer/COGS4290_DataMemoryBrains/dask_appendix_logs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where cluster run logs will be stored\n",
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that you need to explicitly shut down this client (`client.shutdown()`) or shut down your jupyter kernel before running this cell again, or you might leave workers \"stranded\" without access to them (since that access is provided by the Client you would overwrite)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have options when setting up your Dask cluster/client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_dask_client_slurm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_per_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_n_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthreads_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprocesses_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madapt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RAM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1500000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlocal_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dashboard_address'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m':51391'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new dask client instance with associated dashboard for\n",
       "monitoring jobs. The default method assumes a very basic use case - that\n",
       "is, embarassingly parallel tasks - in which the user simply wants to speed\n",
       "up independent computations by running them in parallel. There is just one\n",
       "single-threaded process per job. The thread/process parameters are\n",
       "configurable, but will lead to more complicated parallel executions\n",
       "which might be hard to track. Workers might share memory instead of\n",
       "being independent.\n",
       "Leave the defaults unless you have some specific reason to believe another\n",
       "configuration will help with your use case.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "job_name : gives every job this client runs a shared name. You can\n",
       "    subsequentlyadd prefixes to this name for individual jobs.\n",
       "memory_per_job : a string specifying how much memory each job needs.\n",
       "    Ex: \"1GB\" for 50 jobs would request 1GB of memory for each job,\n",
       "    for a total of 50 GB. If any one job exceeds 1GB, you will get a\n",
       "    memory error.\n",
       "max_n_jobs : maximum number of parallel processes (assuming\n",
       "    processes_per_job is 1). Common courtesy dictates that you avoid\n",
       "    running more than 100 jobs and hogging cluster resources (default: 100)\n",
       "threads_per_job : number of threads used per job (default: 1)\n",
       "adapt : boolean determining whether to use adaptive or manual scaling.\n",
       "queue : SLURM queue to use (default: \"RAM\")\n",
       "walltime : timeout for cluster job, after which job is killed. Seconds,\n",
       "    or HH:MM:SS (default : 17 days)\n",
       "local_directory : directory for dask worker space, used internally\n",
       "log_directory : a directory to dump worker log outputs. Dumped to home\n",
       "    directory by default.\n",
       "scheduler_options : dict of arguments to pass to Dask scheduler directly.\n",
       "    See docs for more\n",
       "    info: https://docs.dask.org/en/latest/how-to/deploy-dask/python-advanced.html#distributed.Scheduler\n",
       "**kwargs\n",
       "\n",
       "Returns\n",
       "-------\n",
       "client : instance of dask.Client\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/src/cmldask/cmldask/CMLDask.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da.new_dask_client_slurm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel computing tips\n",
    "\n",
    "* Jobs are still subject to memory limitations, so you may need to **break up large processes into smaller chunks.** For example, each job could correspond to analyzing one session, instead of one subject (this should be done, e.g., for all Morlet wavelet analyses). \n",
    "* It is often useful to save the output of each job in a dedicated directory, and sometimes useful to save intermediate values to aid in debugging or later nonparallel analyses. The Python \"os\" library can be helpful here. \n",
    "* **Be respectful!** There are only so many computational resources available to your classmates, the Kahana lab, and our collaborators across the country.\n",
    "* **Limit the memory usage of jobs**. Course students should confine parallel jobs to 20 in parallel or 100GB, whichever comes first! So if you reserve 10GB per process, that means only running 10 in parallel. Or, 5GB per process would mean 20 in parallel. Set your job launching parameters accordingly on all jobs!\n",
    "* You can always use the '**qdel**' command in Terminal, followed by your job number, to kill any of your old jobs that may be wasting rhino's resources. \n",
    "* Use the '**squeue**' command in Terminal to see cluster usage information for all users. The **'squeue | grep your_username'** command will just show the jobs associated with your username, but requires you to use only the first 8 characters of your username for \"your_username\".\n",
    "* Search online for other slurm commands you can utilize if you want to see additional information. This is all very well documented.\n",
    "* Each rhino2 node has ~128 GB of memory and ~40 cores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dask Futures Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c78ab49e4864d71a7117b3fab1eaec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "# functions to run on the cluster\n",
    "def add(a, b):\n",
    "    sleep(np.random.randint(0, 10))\n",
    "    return a + b\n",
    "\n",
    "def error_add(a, b):\n",
    "    sleep(np.random.randint(0, 10))\n",
    "    if a % 2:\n",
    "        raise ValueError\n",
    "    return a + b\n",
    "\n",
    "# launch the jobs\n",
    "futures = client.map(add, range(40), range(40))\n",
    "progress(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if first_run:\n",
    "    import time\n",
    "    time.sleep(30) # Give slurm a chance to create files on the first run\n",
    "\n",
    "# just for illustration; the log files are just text files, so it's typically easier to open them up manually\n",
    "with open(os.path.join(logdir, os.listdir(logdir)[0]), 'r') as f:\n",
    "    log = f.read()\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mresources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpriority\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallow_other_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfifo_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'100 ms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Map a function on a sequence of arguments\n",
       "\n",
       "Arguments can be normal objects or Futures\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : callable\n",
       "    Callable to be scheduled for execution. If ``func`` returns a coroutine, it\n",
       "    will be run on the main event loop of a worker. Otherwise ``func`` will be\n",
       "    run in a worker's task executor pool (see ``Worker.executors`` for more\n",
       "    information.)\n",
       "iterables : Iterables\n",
       "    List-like objects to map over.  They should have the same length.\n",
       "key : str, list\n",
       "    Prefix for task names if string.  Explicit names if list.\n",
       "workers : string or iterable of strings\n",
       "    A set of worker hostnames on which computations may be performed.\n",
       "    Leave empty to default to all workers (common case)\n",
       "retries : int (default to 0)\n",
       "    Number of allowed automatic retries if a task fails\n",
       "resources : dict (defaults to {})\n",
       "    Defines the `resources` each instance of this mapped task requires\n",
       "    on the worker; e.g. ``{'GPU': 2}``.\n",
       "    See :doc:`worker resources <resources>` for details on defining\n",
       "    resources.\n",
       "priority : Number\n",
       "    Optional prioritization of task.  Zero is default.\n",
       "    Higher priorities take precedence\n",
       "allow_other_workers : bool (defaults to False)\n",
       "    Used with `workers`. Indicates whether or not the computations\n",
       "    may be performed on workers that are not in the `workers` set(s).\n",
       "fifo_timeout : str timedelta (default '100ms')\n",
       "    Allowed amount of time between calls to consider the same priority\n",
       "actor : bool (default False)\n",
       "    Whether these tasks should exist on the worker as stateful actors.\n",
       "    See :doc:`actors` for additional details.\n",
       "actors : bool (default False)\n",
       "    Alias for `actor`\n",
       "pure : bool (defaults to True)\n",
       "    Whether or not the function is pure.  Set ``pure=False`` for\n",
       "    impure functions like ``np.random.random``. Note that if both\n",
       "    ``actor`` and ``pure`` kwargs are set to True, then the value\n",
       "    of ``pure`` will be reverted to False, since an actor is stateful.\n",
       "    See :ref:`pure functions` for more details.\n",
       "batch_size : int, optional (default: just one batch whose size is the entire iterable)\n",
       "    Submit tasks to the scheduler in batches of (at most)\n",
       "    ``batch_size``.\n",
       "    The tradeoff in batch size is that large batches avoid more per-batch overhead,\n",
       "    but batches that are too big can take a long time to submit and unreasonably delay\n",
       "    the cluster from starting its processing.\n",
       "**kwargs : dict\n",
       "    Extra keyword arguments to send to the function.\n",
       "    Large values will be included explicitly in the task graph.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> L = client.map(func, sequence)  # doctest: +SKIP\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The current implementation of a task graph resolution searches for occurrences of ``key``\n",
       "and replaces it with a corresponding ``Future`` result. That can lead to unwanted\n",
       "substitution of strings passed as arguments to a task if these strings match some ``key``\n",
       "that already exists on a cluster. To avoid these situations it is required to use unique\n",
       "values if a ``key`` is set manually.\n",
       "See https://github.com/dask/dask/issues/9969 to track progress on resolving this issue.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "List, iterator, or Queue of futures, depending on the type of the\n",
       "inputs.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Client.submit : Submit a single function\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/client.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(error_add, range(40), range(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for all jobs to finish, check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>traceback_obj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8bc1c0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb887e80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb87ecc0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8d69c0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e0b80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8d4c80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8d4480&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8bee80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8bffc0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e3b00&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e0900&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e6380&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e3f40&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e5d40&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8f1b40&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8f2ac0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb90e9c0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb919e80&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb8e4b40&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2abeeb91ba40&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          exception                         traceback_obj\n",
       "param                                                    \n",
       "1      ValueError()  <traceback object at 0x2abeeb8bc1c0>\n",
       "3      ValueError()  <traceback object at 0x2abeeb887e80>\n",
       "5      ValueError()  <traceback object at 0x2abeeb87ecc0>\n",
       "7      ValueError()  <traceback object at 0x2abeeb8d69c0>\n",
       "9      ValueError()  <traceback object at 0x2abeeb8e0b80>\n",
       "11     ValueError()  <traceback object at 0x2abeeb8d4c80>\n",
       "13     ValueError()  <traceback object at 0x2abeeb8d4480>\n",
       "15     ValueError()  <traceback object at 0x2abeeb8bee80>\n",
       "17     ValueError()  <traceback object at 0x2abeeb8bffc0>\n",
       "19     ValueError()  <traceback object at 0x2abeeb8e3b00>\n",
       "21     ValueError()  <traceback object at 0x2abeeb8e0900>\n",
       "23     ValueError()  <traceback object at 0x2abeeb8e6380>\n",
       "25     ValueError()  <traceback object at 0x2abeeb8e3f40>\n",
       "27     ValueError()  <traceback object at 0x2abeeb8e5d40>\n",
       "29     ValueError()  <traceback object at 0x2abeeb8f1b40>\n",
       "31     ValueError()  <traceback object at 0x2abeeb8f2ac0>\n",
       "33     ValueError()  <traceback object at 0x2abeeb90e9c0>\n",
       "35     ValueError()  <traceback object at 0x2abeeb919e80>\n",
       "37     ValueError()  <traceback object at 0x2abeeb8e4b40>\n",
       "39     ValueError()  <traceback object at 0x2abeeb91ba40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait(futures)\n",
    "errors = da.get_exceptions(futures, range(40))\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick out the index where you want to view the traceback message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/tmp/ipykernel_11763/762868183.py\", line 12, in error_add\n",
      "    raise ValueError\n"
     ]
    }
   ],
   "source": [
    "da.print_traceback(errors, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice gathering these doesn't work because there are errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36merror_add\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m sleep(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's filter for successful ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_futures = da.filter_futures(futures)\n",
    "client.gather(good_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Shutdown your client (or restart your kernel, which will do so automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 01:12:17,587 - distributed.core - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/utils.py\", line 803, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/scheduler.py\", line 7144, in feed\n",
      "    state = await state\n",
      "            ^^^^^^^^^^^\n",
      "  File \"/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/diagnostics/progressbar.py\", line 293, in setup\n",
      "    await p.setup()\n",
      "  File \"/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/site-packages/distributed/diagnostics/progress.py\", line 203, in setup\n",
      "    await asyncio.sleep(0.05)\n",
      "  File \"/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/lib/python3.11/asyncio/tasks.py\", line 639, in sleep\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "2023-09-25 01:12:17,598 - distributed.deploy.adaptive_core - INFO - Adaptive stop\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recommended Best Practices\n",
    "\n",
    "1. Watch your memory usage. Jobs that go over memory limits will error out. Delete unnecessary variables or overwrite variables to reduce memory overhead.\n",
    "2. Saving out **dimensionally reduced** intermediate outputs. \n",
    "    * Clusters can be unreliable and have outages, and jobs can get killed. Saving results from each job ensures that all results will not lost if a run dies partway through. \n",
    "    * You can write code to check if the results for a particular job have successfully returned and only recompute results for failed jobs.\n",
    "3. Avoid heavy disk usage by reading from and writing to a smaller number of medium to large-sized files rather than a large number of small files. The network/disk bandwidth on Rhino is a major bottleneck. If you use file IO (Input/Output, i.e., reading and writing to files) heavily, it will not only makes others' work slow, it can slow down your own analyses substantially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code with the Pickle library, which allows for saving and loading (most) objects in Python\n",
    "# the pickle library is not recommended for long-term storage since it depends on having the same object \n",
    "# code/package versions available when loading an object that were present when the object was saved\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "arr = np.arange(1000)\n",
    "\n",
    "# save output\n",
    "with open('arr_test.pkl', 'wb') as f:\n",
    "    pickle.dump(arr, f)\n",
    "    \n",
    "# load output\n",
    "with open('arr_test.pkl', 'rb') as f:\n",
    "    arr_loaded = pickle.load(f)\n",
    "\n",
    "# confirm that loaded == saved\n",
    "np.all(arr == arr_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings():\n",
    "  '''settings = Settings()\n",
    "     settings.somelist = [1, 2, 3]\n",
    "     settings.importantstring = 'saveme'\n",
    "     settings.Save()\n",
    "\n",
    "     settings = Settings.Load()\n",
    "  '''\n",
    "  def __init__(self, **kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "      self.__dict__[k] = v\n",
    "\n",
    "  def Save(self, filename='settings.pkl'):\n",
    "    import pickle\n",
    "    with open(filename, 'wb') as fw:\n",
    "      fw.write(pickle.dumps(self))\n",
    "\n",
    "  def Load(filename='settings.pkl'):\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "  def __repr__(self):\n",
    "    return ('Settings(' +\n",
    "      ', '.join(str(k)+'='+repr(v) for k,v in self.__dict__.items()) +\n",
    "      ')')\n",
    "\n",
    "  def __str__(self):\n",
    "    return '\\n'.join(str(k)+': '+str(v) for k,v in self.__dict__.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: FR1\n",
      "logbase: totalwords_\n",
      "sub_list: ['R1341T', 'R1391T', 'R1395M', 'R1467M']\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.experiment = 'FR1'\n",
    "settings.logbase = 'totalwords_'\n",
    "settings.sub_list = ['R1341T', 'R1391T', 'R1395M', 'R1467M']\n",
    "\n",
    "settings.Save('totalwords.pkl')\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmldask.CMLDask as da\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "from cmlreaders import CMLReader, get_data_index\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "def TotalWords(sub):\n",
    "  logfile = f'{settings.logbase}{sub}.txt'\n",
    "  \n",
    "  df = get_data_index('all')\n",
    "  df = df[df['subject']==sub]\n",
    "  df = df[df['experiment']==settings.experiment]\n",
    "\n",
    "  total_words = 0\n",
    "  for df_sess in df.itertuples():\n",
    "    try:\n",
    "      df_sess = df_sess._asdict()\n",
    "      # Get parameters from the session DataFrame\n",
    "      exp = df_sess['experiment']\n",
    "      sess = df_sess['session']\n",
    "      # Prepare the reader\n",
    "      reader = CMLReader(sub, exp, sess, df_sess['montage'], df_sess['localization'])\n",
    "      # Get word events\n",
    "      evs = reader.load('task_events')\n",
    "      word_evs = evs[evs['type']=='WORD']\n",
    "      # Do our analysis!\n",
    "      total_words += len(word_evs)\n",
    "    except Exception as e:\n",
    "      # Log the exception to a subject-labeled filename,\n",
    "      # along with a label of subject, experiment, and session.\n",
    "      with open(logfile, 'a') as fw:\n",
    "        date = datetime.datetime.now().strftime('%F_%H-%M-%S')\n",
    "        fw.write(f'{date}: {sub}, {exp}, {sess}\\n' +\n",
    "                 ''.join(traceback.format_exception(type(e), e, e.__traceback__)))\n",
    "\n",
    "    \n",
    "  # Save the result.\n",
    "  np.save('wordcount_'+sub+'.npy', [total_words])\n",
    "  return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for ryan.colyer is 51391\n",
      "{'dashboard_address': ':51391'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN ryan.colyer@rhino2.psych.upenn.edu -L 8000:192.168.86.143:51391` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to localhost:8000 in your browser\n"
     ]
    }
   ],
   "source": [
    "# Run in parallel\n",
    "client = da.new_dask_client_slurm(\"test_dask\", \"4GB\", max_n_jobs=5)\n",
    "futures = client.map(TotalWords, settings.sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "None of the given futures resulted in exceptions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Wait for results, check errors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wait(futures)\n\u001b[0;32m----> 3\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m errors\n",
      "File \u001b[0;32m/usr/global/miniconda/py310_23.1.0-1/envs/workshop_311/src/cmldask/cmldask/CMLDask.py:231\u001b[0m, in \u001b[0;36mget_exceptions\u001b[0;34m(futures, params)\u001b[0m\n\u001b[1;32m    223\u001b[0m         exceptions\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[1;32m    224\u001b[0m             {\n\u001b[1;32m    225\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam\u001b[39m\u001b[38;5;124m\"\u001b[39m: param,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m             }\n\u001b[1;32m    229\u001b[0m         ))\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exceptions):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of the given futures resulted in exceptions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(exceptions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    233\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mException\u001b[0m: None of the given futures resulted in exceptions"
     ]
    }
   ],
   "source": [
    "# Wait for results, check errors.\n",
    "wait(futures)\n",
    "errors = da.get_exceptions(futures, range(40))\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1260, 468, 936, 156]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_futures = da.filter_futures(futures)\n",
    "client.gather(good_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1341T had [1260] words\n",
      "R1391T had [468] words\n",
      "R1395M had [936] words\n",
      "R1467M had [156] words\n"
     ]
    }
   ],
   "source": [
    "settings = Settings.Load('totalwords.pkl')\n",
    "for sub in settings.sub_list:\n",
    "  total_words = np.load('wordcount_'+sub+'.npy')\n",
    "  print(f'{sub} had {total_words} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy parallel computing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Write a parallel function that returns the number of (bipolar) electrodes for the first 20 subjects of the 'r1' RAM dataset (ignore localization and montage details). Run with 5 jobs and 1 core per job. This will require you to integrate the data loading procedures you learned earlier with the dask framework exhibited using the toy examples above**\n",
    "\n",
    "This example is purely for illustration and this task is likely not much faster with multiple nodes than with a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_311",
   "language": "python",
   "name": "workshop_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
