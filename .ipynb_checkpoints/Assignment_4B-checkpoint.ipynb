{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429ff5ed-76f4-40ee-a5dd-7d8771a3392c",
   "metadata": {},
   "source": [
    "# Assignment 4B: Multivariate and Machine Learning Analysis fo Intracranial EEG Data\n",
    "Please submit this assignment to Canvas as a jupyter notebook (.ipynb).  The assignment will have you utilize machine learning techniques to classify memory states.  Please double click on this cell and edit it to include your name.\n",
    "## Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1e9298-f81c-4f78-b88c-3b63897c2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmlreaders as cml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a806b-948a-42f0-85a3-455291fe16fd",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "This project is designed to familiarize you with multivariate analysis of intracranial EEG data. For each subject, you will train a logistic-regression classifier to discriminate subsequently recalled vs non-recalled studied items using the distribution of spectral power across electrodes as the features. After completing the assignment you should be able to do the following:\n",
    "* Part 1: Fit an L2-penalized logistic regression classifier to intracranial electrophysiological recordings. Construct a receiver operating characteristic (ROC) curve and compute area under the curve (AUC) to assess classifier performance. Compare train and test performance.\n",
    "* Part 2: Optimize model penalization parameters using nested cross-validation, specifically focusing on L2 penalization. Compare z-scoring feate\n",
    "* Part 3: Compare and contrast the performances of classifiers with different penalization schemes such as L1, L2, and elastic net.\n",
    "\n",
    "For Parts 1 and 2, use data from the following 20 FR1/catFR1 subjects in the intracranial EEG (iEEG) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18ba0c4-e928-4414-94a8-5e4f75ab0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['R1380D', 'R1380D', 'R1111M', 'R1332M', 'R1377M', \n",
    "        'R1065J', 'R1385E', 'R1189M', 'R1108J', 'R1390M', \n",
    "        'R1236J', 'R1391T', 'R1401J', 'R1361C', 'R1060M', \n",
    "        'R1350D', 'R1378T', 'R1375C', 'R1383J', 'R1354E', \n",
    "        'R1292E']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdcaf5-d1db-434a-80d2-5033b6550f65",
   "metadata": {},
   "source": [
    "All analyses in Part 1 should be performed on all 20 iEEG subjects. For each of these subjects, use the following processing steps:\n",
    "* Load EEG with CMLReader.load_eeg from a bipolar montage loaded using CMLReader.load('pairs').\n",
    "* Apply a Butterworth notch filter around 60 Hz (freqs = [58 62]) when extracting the voltage.\n",
    "* Calculate power at the above frequencies with a Morlet wavelet with wavenumber (keyword “width”) of 6 for each encoding event (from time 0 until 1.6 seconds after the encoding event onset) using a 1 second buffer.\n",
    "* For each frequency, channel, and encoding event, average the power over the entire 1600 ms encoding period (but not over the buffer period!)\n",
    "* Log-transform the average encoding power values as in the final step of the previous problem.\n",
    "* In some cases you may notice artifacts in the data that manifest in power values of zero. These would produce problems in the transformation and classification, so please exclude any events with this issue from all analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a3794-6e26-4f86-9487-568839485d25",
   "metadata": {},
   "source": [
    "## Part I: Generating Features, Cross-Validation, ROCs and AUCs\n",
    "For the first part of this assignment, you will train an L2-penalized logistic regression classifier on the time-frequency (TF) data obtained during item encoding for every subject. Throughout this assignment, unless otherwise specified, we will use the default parameters for the *LogisticRegression* classifier in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a081e01-a6c6-4218-a06c-2588e2b9af35",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "1) For each subject, create an $X_{N×p}$ matrix of spectral power patterns ($N$= number of encoding events concatenated across sessions; $p$ = number of frequencies × number of channels) and obtain the $y_{N×1}$ vector of labels (1: recalled, 0: non-recalled). The pair $(X, y)$ will be our dataset.\n",
    "* For your input features, extract spectral power with Morlet wavelets at 8 frequencies logarithmically spaced between 3 and 180 Hz (np.logspace(np.log10(3), np.log10(180),8)) for each recorded electrode pair (“channel”). Average the power across each of these frequencies over the 1600 ms word encoding period.\n",
    "* Z-score the features across observations (i.e., events) within each session. Since we're performing leave-one-session-out cross validation in this assignment, z-scoring within-session prevents information from leaking between train sessions and test sessions through the z-score statistics.\n",
    "* Some subjects will have different sets of electrodes for different recording sessions. For these subjects you can drop the sessions such that you keep the largest possible set of available sessions which all have the same recording contacts (there could be groups of sessions for the same subject with different electrode sets). The reasons a subject might have different active recording electrodes across sessions are:\n",
    "    * some subjects have so many electrodes implanted that not all of them could be recorded from simultaneously; these subjects will sometimes then have different \"montages\" in which different electrodes are turned connected or disconnected or\n",
    "    * some subjects have multiple implant surgeries, with different electrodes being in place after each operation, meaning again that the same subject will have different sets of electrodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb57362-89b5-4685-ac9b-52f6f89f5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cca76a-bc50-4596-9b69-246c69ec4f2b",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "1) Use leave-one-session-out cross-validation to train and test L2-penalized logistic regression classifiers.\n",
    "\n",
    "* For each cross-validation iteration, you will (1) leave out one session ($X_{test}$, $y_{test}$), (2) train the model on the other sessions ($X_{train}$, $y_{train}$), and (3) test the trained model on the held-out session. Repeat this procedure by iterating across all sessions that a subject has. For each iteration of the cross-validation procedure, you will train the L2-penalized logistic regression classifier on the encoding events from all sessions except the held-out session. You will take the model fit to this training set and use it to predict recall performance for the encoding events in the held-out session. For each encoding event in the held out session, you should get a predicted probability that this item will be subsequently recalled. Once you have held out each session (i.e., at the end of the leave-one-session-out cross-validation procedure), you will have the predicted probability for each encoding event (all predicted by models trained on all encoding events except for the ones in the same session). After doing the above separately for each subject, you should now have cross-validated predictions for all encoding events. Use the default penalty parameter (C) of 1.0 (you will optimize this parameter for some subjects in Part 2). \n",
    "\n",
    "2) For the first three subjects in the list above, plot a histogram of the predicted cross-validated probabilities across all encoding events, giving different colors to predictions for encoding events of words that were subsequently recalled and for encoding events for words that were not recalled. \n",
    "\n",
    "3) Based on your results from (1) and the visualizations in (2), how strongly do the neural features predict subsequent recall?\n",
    "\n",
    "* Hint: since different sessions have different numbers of events (subjects can discontinue a session partway through), you'll need to implement leave-one-session-out cross validation without using the KFold sklearn class used in the examples from the intro material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346a8dfd-4d0e-4b3b-8f84-2561ba076243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3053857b-d468-4a70-b1c4-02a7a1c34401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2.2\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00a594-9ec7-4737-88c3-a548c82a6d21",
   "metadata": {},
   "source": [
    "Question 2.3\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724103e-2c15-4f7b-919f-5071360de0c1",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "* To assess the performance of a classifier, we will utilize the area under the receiver operating curve (AUC). \n",
    "\n",
    "1) Using sklearn’s ROC curve function, calculate the ROC and the corresponding AUC for each subject. Plot all the subjects’ ROC curves in one plot, and plot all the subjects’ AUCs in one histogram. \n",
    "\n",
    "2) Compute a subject-level ROC curve and AUC value, by pooling all predictions across the outer cross-validation folds and compute the AUC/ROC curve with the pooled predictions.\n",
    "\n",
    "3) How good is the performance? Run a statistical test to determine if the between-subject average performance is reliably above chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af8c37a-db84-4c52-a1f0-627c3a5ab82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0769957a-d8a0-4b3c-873f-4f5c813eab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3.2\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7908c0-fde6-4dfe-9c89-b138c7b535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3.3\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f857ac-c1ca-4974-95ee-bfa7ce03c843",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "1) Report mean train AUCs and mean test AUCs across cross validation folds for all subjects with two overlapping histograms (two histograms in the same plot). \n",
    "* In comparison to how the test AUCs were computed at the subject level in previous problems, you can compute the train and test AUCs for a given outer fold with just the predictions from that fold; then you can average those fold-level AUCs together.\n",
    "2) What is the mean difference across subjects in cross-validated AUC scores between training and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2286e69-8d0b-46a9-9e03-c5935321842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364786c-2b43-4475-838f-915befa2c21c",
   "metadata": {},
   "source": [
    "Question 4.2\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b150a-ffdc-47d0-a804-6b58135dd5d8",
   "metadata": {},
   "source": [
    "## Part II: Optimizing Hyperparameters using Nested Cross-Validation\n",
    "\n",
    "In the previous part you used a fixed penalty parameter (C = 1) for all subjects. Oftentimes we want to tune the hyperparameters of a model to optimize its performance. It is crucial to make it clear that the aim of cross-validation is not to get one or multiple trained models for inference, but to estimate an unbiased generalization performance. We can use a grid search approach in which we search over a grid of 10 values of C logarithmically spaced between $10^{−6}$ and $10^2$ (np.logspace(-6,2,10)). One naive approach is to repeat Part I for every C value and select the optimal C that maximizes the average AUC across folds. The problem is, if we use the test set multiple times for different trained models, during our selection of the optimal model, the test set actually “leaks” information, and is thus impure or biased. To rigorously select the optimal parameter C and correctly estimate the prediction error of the optimal model, we utilize a nested cross-validation procedure. As the name suggests, you will perform two rounds of cross-validation with the inner CV nested in the outer CV. The outer CV is responsible for obtaining the prediction error for the model and the inner CV is responsible for selecting the optimal hyperparameter for each outer CV fold. We apply the nested cross-validation procedure to our dataset as follows:\n",
    "* For each subject, divide the dataset into K folds corresponding to K sessions.\n",
    "* For each fold k = 1, · · · , K (outer loop for evaluation of the model):\n",
    "    * Let *test* be the kth session (hold-out fold), and *train_val* be all other sessions except the kth fold. *train_val* should have K − 1 sessions. We next perform cross-validation on the train-validation data (inner CV), while leaving the test data alone.\n",
    "    * For each l = 1, · · · , K − 1 (inner loop for hyperparameter tunning):\n",
    "        - Let *val_inner* be the held-out fold for the inner CV, and *train_inner* be the other K − 2 sessions except for the lth fold.\n",
    "        - For each value C in the grid, train a classifier on the *train_inner* data set and obtain prediction for the inner held-out fold, *val_inner*.\n",
    "        - Repeat the procedure until you sweep through every session of trainval.\n",
    "    * Select the optimal C from the inner cross-validation that maximizes the inner AUC for the *train_val* data.\n",
    "    * Retrain the model using the entire *train_val* data with the optimal C.\n",
    "    * Finally, test the optimal model on the outer held-out fold K.\n",
    "* Repeat the procedure, holding out each fold in turn as the test fold. As you notice, the procedure above is computationally intensive and requires a lot of data. For instance, R1065J has 10 sessions of data. We perform 10 iterations for the outer CV and for each held-out session in the outer CV, we perform an inner CV procedure on the outer training data, which entails an addition of 9 iterations. On top of it, we need to perform the inner CV for all 10 values of C in the grid. As a result, you will be training 10 × 9 × 10 = 900 classifiers for a single subject using this procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150fe4a-2e69-4b9e-becf-52ff571850b8",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "1) Perform nested CV on subjects with at least 6 sessions of data and compare the performance (AUC) of the optimal classifier (with the optimal C) to the default classifier (with C = 1.0) using L2 penalized logistic regression.\n",
    "\n",
    "2) Does optimizing the regularization hyperparameter help? Use barplots, scatterplots, and appropriate statistical tests to support your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3341bdb-dcbe-434f-bc9c-ac945e4ff684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9832b9f2-fc08-4a0b-b66d-d83a22ecd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.2\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf9db5-8ae0-4018-85a2-d2683a2c0619",
   "metadata": {},
   "source": [
    "## Part 3: Comparison between different penalization schemes and z-scoring features.\n",
    "\n",
    "In this section you will investigate how different penalization schemes and z-scoring features can produce different behaviors in the classifier. Recall that the objective function for penalized logistic regression is: \n",
    "\n",
    "$l(\\beta) = \\Sigma_{i=1}^N{y_i log p_i + (1 − y_i) log(1 − p_i)} + \\frac{\\alpha}{2} r||\\beta||_2^2 + \\alpha(1 − r)||\\beta||_1$\n",
    "\n",
    "where, $\\alpha = 1/C$ is the penalty parameter, r is the contribution of L2 penalty, and 1 − r is the contribution of L1 penalty. When r = 1, we have a strictly L2 penalized logistic regression. When r = 0, we have a strictly L1 penalized regression (a.k.a. Lasso). When 0 < r < 1, we have a mixture of both L1 and L2, which is called elastic net. In this part, you will compare the performances of different penalization schemes: strictly L2, strictly L1, and elastic net with r = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cb7c6-4b5e-4558-87ad-39e46c56b9a7",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "1) Repeat the nested cross-validation procedure in Part 2 using z-scored features for L2, L1, and elastic net. \n",
    "\n",
    "* You should again use sklearn’s linear_model.LogisticRegression class for your classifier, appropriately selecting the classifier hyperparameters to obtain the L1 and elastic net regularization schemes.\n",
    "\n",
    "2) Compare the performances (AUCs) across these three schemes using a barplot or whatever you see fit, including some visualization of variability in the outcomes for these methods. Does one scheme do better than the others?\n",
    "\n",
    "* Keep in mind that the sklearn LogisticRegression parameter `C` is an inverse regularization strength, meaning the regularization strength is equal to $\\alpha$ = 1/C. So higher `C` means lower regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27b54d4-29d2-4f3f-bd8a-3abadbec960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff89a7d-de26-4c47-9995-2a8b90944da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.2\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84e71b-bdb2-4167-aeba-6b3d467626c5",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "1) Generate a plot for each of the first three subjects containing three histograms (one for each penalization scheme) of the model coefficients.\n",
    "* Use the *model.coef_* attribute (*model* is your classifier object) to investigate the learned coefficients of the classifier for each subject. You can pool classifier weights across all outer cross-validation folds. \n",
    "* Use the *alpha* parameter of the plt.hist function to ensure the histograms do not cover each other up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e34c9d7-8a55-430a-9a75-70b7c8dd9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5efcfe-e18e-4148-87b3-91ccb14c7213",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "It has been shown that L1 penalization introduces sparsity to classifier weights, i.e. some of the β’s in the model will be zero with L1. \n",
    "\n",
    "1) For each subject, report the proportion of non-zero β’s for that subject’s classifier weights pooled across outer cross-validation folds separately for these three schemes. Plot a histogram of these subject proportions for each penalization scheme. \n",
    "\n",
    "2) What can you say about the proportions of non-zero β’s across the three penalization schemes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f57f21f1-61b3-4bcb-ac88-ea5e4878879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38b08c-404c-4d09-9db9-8c37abd1db97",
   "metadata": {},
   "source": [
    "Question 8.2\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79966a3-33b0-4de9-a5f7-571556c461b6",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "1) Test whether z-scoring improves classifier performance by repeating the analysis from Part 2 but without z-scored features.\n",
    "\n",
    "2) Which is better, raw features or z-scored features?  Give an intuitive explanation as to why one is better than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7c48f48-3746-41d4-927b-5834b72fdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9.1\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4e9e2-f290-43a3-ae25-22258a2cdca6",
   "metadata": {},
   "source": [
    "Question 9.2\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981fa5e-dc68-4e1c-88b6-91c3fc857d23",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "In Part 3, you used a mean nested CV score to compare penalization schemes and to compare z-scored features to raw features. \n",
    "\n",
    "1) What can we conclude about the generalization of these comparisons? Are the improvements you found between these methods biased or unbiased in the sense of overfitting? In other words, if you tested whichever methods among these tested methods that you found achieved the optimal score in a fresh held-out data set, would the method be expected to achieve the same expected performance (assume we had a large enough sample of subjects to ignore subject-level variability)? What would be one scheme you could use to obtain an unbiased estimator of the population-level (as opposed to the individual subject-level) hold-out performance of your chosen optimal methods in new data? \n",
    "\n",
    "2) What about a scheme for an unbiased estimate of the performance of these methods at the individual subject level? In other words, if we wanted to ask \"which penalization method or z-scoring approach is best for each subject separately?\" and then evaluate the performance of the best method for that subject at the individual level, how could we do it in an unbiased manner (without \"cheating\")? Think about the methods you've used so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfdfa8-245f-4653-8485-9c0095b1f6ea",
   "metadata": {},
   "source": [
    "Question 10.1\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01f5fe-5593-4d3a-9137-54d788cbd47d",
   "metadata": {},
   "source": [
    "Question 10.2\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bids",
   "language": "python",
   "name": "bids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
